# 1C Portal RAG System - Complete Setup Guide

## ğŸ“‹ Prerequisites

### 1. Install Python (3.8 or higher)
- Download from: https://www.python.org/downloads/
- During installation, check "Add Python to PATH"

### 2. Install PyCharm IDE
- Download Community Edition (Free): https://www.jetbrains.com/pycharm/download/
- Or use PyCharm Professional if you have a license

### 3. Get OpenAI API Key
1. Go to: https://platform.openai.com/
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
2. Sign up / Login
3. Navigate to: API Keys section
4. Click "Create new secret key"
5. Copy and save the key securely (starts with `sk-proj-...`)

---

## ğŸš€ Step-by-Step Installation

### Step 1: Create Project in PyCharm

1. Open PyCharm
2. File â†’ New Project
3. Project Name: `1C_Portal_RAG_Chatbot`
4. Location: Choose your preferred folder
5. Python Interpreter: Select "New virtual environment"
6. Click "Create"

### Step 2: Install Required Libraries

Open PyCharm Terminal (Bottom of PyCharm window) and run these commands ONE BY ONE:

```bash
pip install openai==0.28.0
pip install faiss-cpu
pip install PyPDF2
pip install numpy
pip install pickle5
```

**Wait for each installation to complete before running the next command.**

#### Verify Installation:
```bash
pip list
```

You should see all installed packages listed.

---

## ğŸ“ Project Structure

Create this folder structure in your PyCharm project:

```
1C_Portal_RAG_Chatbot/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ 1C_Portal_Support_Guide_v3.2.pdf  (Your PDF file)
â”‚
â”œâ”€â”€ vector_db/
â”‚   â”œâ”€â”€ vectors.index  (Will be generated)
â”‚   â””â”€â”€ chunks.pkl     (Will be generated)
â”‚
â”œâ”€â”€ config.py          (Create this)
â”œâ”€â”€ pdf_to_vectors.py  (Create this)
â”œâ”€â”€ ask_questions.py   (Create this)
â”œâ”€â”€ rag_chatbot.py     (Create this - Main app)
â””â”€â”€ README.md          (Create this)
```

### Creating Folders in PyCharm:
1. Right-click on project root
2. New â†’ Directory
3. Name it `data` and `vector_db`

---

## ğŸ“ Configuration Files

### File 1: config.py

```python
"""
Configuration file for 1C Portal RAG System
Store your OpenAI API key here
"""

import os

# OpenAI Configuration
OPENAI_API_KEY = "YOUR_API_KEY_HERE"  # Replace with your actual key

# File Paths
PDF_PATH = "data/1C_Portal_Support_Guide_v3.2.pdf"
VECTOR_INDEX_PATH = "vector_db/vectors.index"
CHUNKS_PKL_PATH = "vector_db/chunks.pkl"

# RAG Parameters
CHUNK_SIZE = 500
CHUNK_OVERLAP = 100
EMBEDDING_MODEL = "text-embedding-ada-002"
CHAT_MODEL = "gpt-3.5-turbo"
TOP_K_RESULTS = 5  # Number of relevant chunks to retrieve

# System Prompt
SYSTEM_PROMPT = """You are an AI assistant specialized in helping Cognizant employees with 1C Portal queries.

Your role:
1. Answer questions based ONLY on the provided context from the 1C Portal Support Guide
2. Provide step-by-step instructions when needed
3. Mention page numbers when referencing specific information
4. If information is not in the context, say "I don't have that information in the support guide"
5. Be professional, clear, and concise
6. Format your answers with proper structure (numbered steps, bullet points when appropriate)

Guidelines:
- Always cite the relevant section or page when providing information
- If a user asks about timesheets, leaves, expenses, or projects, provide detailed steps
- For technical issues, suggest troubleshooting steps
- Recommend raising IT tickets when necessary"""
```

**IMPORTANT:** Replace `YOUR_API_KEY_HERE` with your actual OpenAI API key!

---

## ğŸ”§ Core Python Files

### File 2: pdf_to_vectors.py

```python
"""
PDF to Vector Database Converter
This script converts the 1C Portal Support Guide PDF into a searchable vector database
Run this ONCE to create the vector database
"""

import faiss
import openai
import PyPDF2
import numpy as np
import pickle
import os
from config import *

# Set OpenAI API key
openai.api_key = OPENAI_API_KEY


def pdf_to_vectors(pdf_path):
    """
    Convert PDF to vector embeddings and save to FAISS index
    
    Args:
        pdf_path: Path to the PDF file
    
    Returns:
        embeddings: numpy array of embeddings
        chunks: list of text chunks
    """
    
    print("="*70)
    print("ğŸ“š 1C PORTAL RAG SYSTEM - VECTOR DATABASE CREATION")
    print("="*70)
    
    # Check if PDF exists
    if not os.path.exists(pdf_path):
        print(f"âŒ ERROR: PDF file not found at {pdf_path}")
        print("ğŸ“ Please place your PDF in the 'data' folder")
        return None, None
    
    # Read PDF
    print(f"\nğŸ“„ Reading PDF: {pdf_path}")
    try:
        with open(pdf_path, 'rb') as f:
            pdf_reader = PyPDF2.PdfReader(f)
            total_pages = len(pdf_reader.pages)
            
            # Extract text from each page
            page_texts = []
            for page_num, page in enumerate(pdf_reader.pages):
                print(f"   Reading page {page_num + 1}/{total_pages}...", end='\r')
                page_text = page.extract_text()
                page_texts.append({
                    'text': page_text,
                    'page_number': page_num + 1
                })
            
            # Combine all text
            full_text = '\n'.join([p['text'] for p in page_texts])
        
        print(f"\nâœ… PDF read successfully!")
        print(f"ğŸ“Š Total pages: {total_pages}")
        print(f"ğŸ“Š Total characters: {len(full_text):,}")
        print(f"ğŸ“Š Average chars/page: {len(full_text) // total_pages:,}")
        
    except Exception as e:
        print(f"âŒ Error reading PDF: {str(e)}")
        return None, None
    
    # Create chunks with better context preservation
    print(f"\nâœ‚ï¸  Creating text chunks...")
    chunks = []
    chunk_metadata = []
    
    # Smart chunking: preserve paragraphs when possible
    for page_info in page_texts:
        page_text = page_info['text']
        page_num = page_info['page_number']
        
        # Split page into chunks
        for i in range(0, len(page_text), CHUNK_SIZE - CHUNK_OVERLAP):
            chunk_text = page_text[i:i + CHUNK_SIZE]
            
            if len(chunk_text.strip()) > 50:  # Skip very small chunks
                chunks.append(chunk_text)
                chunk_metadata.append({
                    'page_number': page_num,
                    'chunk_index': len(chunks) - 1,
                    'char_start': i,
                    'char_end': i + len(chunk_text)
                })
    
    print(f"âœ… Created {len(chunks)} chunks")
    print(f"ğŸ“Š Average chunk size: {sum(len(c) for c in chunks) // len(chunks)} characters")
    
    # Get embeddings from OpenAI
    print(f"\nğŸ”„ Generating embeddings using OpenAI ({EMBEDDING_MODEL})...")
    print("â³ This may take a few minutes...")
    
    embeddings = []
    failed_chunks = 0
    
    for i, chunk in enumerate(chunks):
        try:
            print(f"   Processing chunk {i + 1}/{len(chunks)} ({(i+1)/len(chunks)*100:.1f}%)...", end='\r')
            
            response = openai.Embedding.create(
                input=chunk,
                model=EMBEDDING_MODEL
            )
            embeddings.append(response['data'][0]['embedding'])
            
        except Exception as e:
            print(f"\nâš ï¸  Warning: Failed to embed chunk {i + 1}: {str(e)}")
            failed_chunks += 1
            # Use zero vector as placeholder
            embeddings.append([0.0] * 1536)
    
    print(f"\nâœ… Embeddings generated!")
    if failed_chunks > 0:
        print(f"âš ï¸  {failed_chunks} chunks failed to embed")
    
    # Create FAISS index
    print(f"\nğŸ—‚ï¸  Creating FAISS vector index...")
    embeddings_array = np.array(embeddings).astype('float32')
    
    # Normalize vectors for better similarity search
    faiss.normalize_L2(embeddings_array)
    
    # Create index with inner product (cosine similarity for normalized vectors)
    index = faiss.IndexFlatIP(1536)  # OpenAI embeddings are 1536 dimensions
    index.add(embeddings_array)
    
    print(f"âœ… FAISS index created with {index.ntotal} vectors")
    
    # Create vector_db directory if it doesn't exist
    os.makedirs(os.path.dirname(VECTOR_INDEX_PATH), exist_ok=True)
    
    # Save to files
    print(f"\nğŸ’¾ Saving vector database...")
    
    try:
        # Save FAISS index
        faiss.write_index(index, VECTOR_INDEX_PATH)
        print(f"âœ… Saved: {VECTOR_INDEX_PATH}")
        
        # Save chunks and metadata
        with open(CHUNKS_PKL_PATH, "wb") as f:
            pickle.dump({
                'chunks': chunks,
                'metadata': chunk_metadata,
                'total_pages': total_pages,
                'pdf_name': os.path.basename(pdf_path),
                'embedding_model': EMBEDDING_MODEL
            }, f)
        print(f"âœ… Saved: {CHUNKS_PKL_PATH}")
        
    except Exception as e:
        print(f"âŒ Error saving files: {str(e)}")
        return None, None
    
    # Summary
    print("\n" + "="*70)
    print("ğŸ‰ VECTOR DATABASE CREATED SUCCESSFULLY!")
    print("="*70)
    print(f"ğŸ“ Files created:")
    print(f"   â€¢ {VECTOR_INDEX_PATH}")
    print(f"   â€¢ {CHUNKS_PKL_PATH}")
    print(f"\nğŸ“Š Statistics:")
    print(f"   â€¢ Total pages processed: {total_pages}")
    print(f"   â€¢ Total chunks created: {len(chunks)}")
    print(f"   â€¢ Vector dimensions: 1536")
    print(f"   â€¢ Index size: {index.ntotal} vectors")
    print(f"   â€¢ Average chunks per page: {len(chunks) / total_pages:.1f}")
    print("\nâœ… You can now run 'rag_chatbot.py' to start chatting!")
    print("="*70)
    
    return embeddings_array, chunks


if __name__ == "__main__":
    # Convert PDF to vectors
    embeddings, chunks = pdf_to_vectors(PDF_PATH)
    
    if embeddings is not None:
        print("\nâœ¨ Setup complete!")
        print("â–¶ï¸  Next step: Run 'rag_chatbot.py' to start the chatbot")
    else:
        print("\nâŒ Setup failed. Please check the errors above.")
```

---

### File 3: ask_questions.py

```python
"""
Question Answering Module
Handles querying the vector database and generating answers
"""

import faiss
import openai
import numpy as np
import pickle
import os
from config import *

# Set OpenAI API key
openai.api_key = OPENAI_API_KEY


def load_vector_database():
    """
    Load the vector database and chunks
    
    Returns:
        index: FAISS index
        chunks: list of text chunks
        metadata: chunk metadata
        total_pages: number of pages in original PDF
    """
    
    # Check if vector files exist
    if not os.path.exists(VECTOR_INDEX_PATH) or not os.path.exists(CHUNKS_PKL_PATH):
        print("âŒ Error: Vector database not found!")
        print("ğŸ”§ Please run 'pdf_to_vectors.py' first to create the database.")
        return None, None, None, None
    
    try:
        # Load FAISS index
        index = faiss.read_index(VECTOR_INDEX_PATH)
        
        # Load chunks and metadata
        with open(CHUNKS_PKL_PATH, "rb") as f:
            data = pickle.load(f)
        
        chunks = data['chunks']
        metadata = data['metadata']
        total_pages = data['total_pages']
        
        print(f"âœ… Database loaded: {len(chunks)} chunks from {total_pages} pages")
        
        return index, chunks, metadata, total_pages
        
    except Exception as e:
        print(f"âŒ Error loading database: {str(e)}")
        return None, None, None, None


def search_similar_chunks(question, index, chunks, metadata, top_k=TOP_K_RESULTS):
    """
    Search for similar chunks using semantic search
    
    Args:
        question: User's question
        index: FAISS index
        chunks: List of text chunks
        metadata: Chunk metadata
        top_k: Number of results to return
    
    Returns:
        relevant_chunks: List of relevant text chunks with metadata
    """
    
    try:
        # Get question embedding
        response = openai.Embedding.create(
            input=question,
            model=EMBEDDING_MODEL
        )
        query_vector = np.array(response['data'][0]['embedding']).reshape(1, -1).astype('float32')
        
        # Normalize query vector
        faiss.normalize_L2(query_vector)
        
        # Search similar chunks
        scores, indices = index.search(query_vector, top_k)
        
        # Prepare results
        relevant_chunks = []
        for score, idx in zip(scores[0], indices[0]):
            if idx < len(chunks):  # Validate index
                relevant_chunks.append({
                    'text': chunks[idx],
                    'page_number': metadata[idx]['page_number'],
                    'similarity_score': float(score),
                    'chunk_index': idx
                })
        
        return relevant_chunks
        
    except Exception as e:
        print(f"âŒ Error searching chunks: {str(e)}")
        return []


def generate_answer(question, relevant_chunks, total_pages):
    """
    Generate answer using GPT with relevant context
    
    Args:
        question: User's question
        relevant_chunks: List of relevant chunks with metadata
        total_pages: Total pages in document
    
    Returns:
        answer: Generated answer
        sources: List of source page numbers
    """
    
    try:
        # Build context from relevant chunks
        context_parts = []
        source_pages = set()
        
        for chunk_info in relevant_chunks:
            page_num = chunk_info['page_number']
            chunk_text = chunk_info['text']
            score = chunk_info['similarity_score']
            
            # Only include chunks with reasonable similarity
            if score > 0.5:  # Threshold for relevance
                context_parts.append(f"[Page {page_num}]:\n{chunk_text}")
                source_pages.add(page_num)
        
        if not context_parts:
            return "I couldn't find relevant information in the 1C Portal Support Guide to answer this question. Please try rephrasing or ask about topics covered in the guide (Timesheets, Leave Management, Expense Claims, Project Assignments, etc.).", []
        
        context = '\n\n---\n\n'.join(context_parts)
        
        # Generate answer using GPT
        response = openai.ChatCompletion.create(
            model=CHAT_MODEL,
            messages=[
                {
                    "role": "system",
                    "content": SYSTEM_PROMPT
                },
                {
                    "role": "user",
                    "content": f"""Document Context (from 1C Portal Support Guide - {total_pages} pages total):

{context}

User Question: {question}

Please provide a detailed answer based on the context above. Include specific steps if the question is about a process. Mention relevant page numbers when providing information."""
                }
            ],
            temperature=0.7,
            max_tokens=800
        )
        
        answer = response.choices[0].message.content
        sources = sorted(list(source_pages))
        
        return answer, sources
        
    except Exception as e:
        print(f"âŒ Error generating answer: {str(e)}")
        return f"Sorry, I encountered an error while generating the answer: {str(e)}", []


def ask_question(question, show_debug=False):
    """
    Main function to ask a question and get an answer
    
    Args:
        question: User's question
        show_debug: Whether to show debug information
    
    Returns:
        answer: Generated answer
        sources: Source page numbers
    """
    
    # Load database
    index, chunks, metadata, total_pages = load_vector_database()
    
    if index is None:
        return None, []
    
    # Search for relevant chunks
    if show_debug:
        print(f"\nğŸ” Searching for relevant information...")
    
    relevant_chunks = search_similar_chunks(question, index, chunks, metadata)
    
    if show_debug:
        print(f"ğŸ“Š Found {len(relevant_chunks)} relevant chunks:")
        for i, chunk_info in enumerate(relevant_chunks[:3], 1):
            print(f"   {i}. Page {chunk_info['page_number']} (Score: {chunk_info['similarity_score']:.3f})")
    
    # Generate answer
    if show_debug:
        print(f"\nğŸ’­ Generating answer...")
    
    answer, sources = generate_answer(question, relevant_chunks, total_pages)
    
    return answer, sources


if __name__ == "__main__":
    # Test mode
    print("ğŸ§ª Testing ask_questions module...")
    
    test_question = "How do I fill my timesheet?"
    print(f"\nâ“ Test Question: {test_question}")
    
    answer, sources = ask_question(test_question, show_debug=True)
    
    if answer:
        print(f"\nğŸ¤– Answer:\n{answer}")
        if sources:
            print(f"\nğŸ“„ Sources: Pages {', '.join(map(str, sources))}")
```

---

### File 4: rag_chatbot.py (MAIN APPLICATION)

```python
"""
1C Portal RAG Chatbot - Main Application
Interactive chatbot for querying the 1C Portal Support Guide
"""

import os
import sys
from datetime import datetime
from ask_questions import ask_question, load_vector_database
from config import *


def print_banner():
    """Print welcome banner"""
    banner = """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                              â•‘
â•‘        ğŸ¤– 1C PORTAL SUPPORT CHATBOT                         â•‘
â•‘           Powered by RAG + OpenAI GPT                        â•‘
â•‘                                                              â•‘
â•‘     Your AI assistant for Cognizant 1C Portal queries       â•‘
â•‘                                                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
    print(banner)


def print_help():
    """Print help information"""
    help_text = """
ğŸ“š AVAILABLE COMMANDS:
   â€¢ Type your question naturally (e.g., "How do I fill timesheet?")
   â€¢ 'help' or '?' - Show this help message
   â€¢ 'info' - Show database statistics
   â€¢ 'examples' - Show example questions
   â€¢ 'clear' - Clear screen
   â€¢ 'quit', 'exit', 'bye', 'q' - Exit the chatbot

ğŸ’¡ TIPS:
   â€¢ Be specific in your questions
   â€¢ Use keywords like "timesheet", "leave", "expense", "project"
   â€¢ Ask step-by-step questions for processes
   â€¢ Mention specific scenarios for better answers

ğŸ“‹ TOPICS COVERED:
   â€¢ Timesheet Management
   â€¢ Leave Applications
   â€¢ Expense Claims & Reimbursements
   â€¢ Project Assignments
   â€¢ Performance Management
   â€¢ Learning & Development
   â€¢ Security & Troubleshooting
"""
    print(help_text)


def print_examples():
    """Print example questions"""
    examples = """
ğŸ’¡ EXAMPLE QUESTIONS:

ğŸ“Š Timesheet Related:
   â€¢ "How do I submit my weekly timesheet?"
   â€¢ "What should I do if my project code is not showing?"
   â€¢ "How to fill timesheet for overtime hours?"
   â€¢ "How do I correct a submitted timesheet?"

ğŸ–ï¸ Leave Related:
   â€¢ "How many leave days am I entitled to?"
   â€¢ "What is the process to apply for leave?"
   â€¢ "How do I check my leave balance?"
   â€¢ "Can I cancel an approved leave?"

ğŸ’° Expense Related:
   â€¢ "How do I submit an expense claim?"
   â€¢ "What documents are required for reimbursement?"
   â€¢ "What is the per diem rate for travel?"
   â€¢ "How long does reimbursement take?"

ğŸ“ Project Related:
   â€¢ "How do I request a project extension?"
   â€¢ "Where can I view my current allocations?"
   â€¢ "What should I do during bench time?"

ğŸ”§ Technical Issues:
   â€¢ "My timesheet is not submitting, what should I do?"
   â€¢ "How do I reset my 1C Portal password?"
   â€¢ "I cannot login to the portal"
"""
    print(examples)


def print_info():
    """Print database information"""
    index, chunks, metadata, total_pages = load_vector_database()
    
    if index is None:
        print("âŒ Database not loaded")
        return
    
    info = f"""
ğŸ“Š DATABASE STATISTICS:
   â€¢ PDF Document: 1C Portal Support Guide
   â€¢ Total Pages: {total_pages}
   â€¢ Total Chunks: {len(chunks)}
   â€¢ Vector Dimensions: 1536
   â€¢ Embedding Model: {EMBEDDING_MODEL}
   â€¢ Chat Model: {CHAT_MODEL}
   â€¢ Average Chunks per Page: {len(chunks) / total_pages:.1f}
   â€¢ Vector Index: {os.path.getsize(VECTOR_INDEX_PATH) / 1024:.1f} KB
   â€¢ Chunks Data: {os.path.getsize(CHUNKS_PKL_PATH) / 1024:.1f} KB
"""
    print(info)


def clear_screen():
    """Clear terminal screen"""
    os.system('cls' if os.name == 'nt' else 'clear')


def main():
    """Main chatbot loop"""
    
    # Check if vector database exists
    if not os.path.exists(VECTOR_INDEX_PATH) or not os.path.exists(CHUNKS_PKL_PATH):
        print("âŒ ERROR: Vector database not found!")
        print("\nğŸ“‹ SETUP REQUIRED:")
        print("   1. Place your PDF in the 'data' folder")
        print("   2. Run: python pdf_to_vectors.py")
        print("   3. Then run: python rag_chatbot.py")
        print("\nğŸ’¡ After setup, the chatbot will be ready to answer your questions!")
        return
    
    # Load database and show banner
    clear_screen()
    print_banner()
    
    print("ğŸ”„ Loading vector database...")
    index, chunks, metadata, total_pages = load_vector_database()
    
    if index is None:
        print("âŒ Failed to load database. Please check setup.")
        return
    
    print(f"âœ… Ready! Database loaded with {total_pages} pages and {len(chunks)} chunks")
    print("\nğŸ’¬ Start asking questions about the 1C Portal!")
    print("ğŸ’¡ Type 'help' for commands or 'examples' for sample questions")
    print("="*70)
    
    # Chat loop
    conversation_count = 0
    
    while True:
        try:
            # Get user input
            print()
            question = input("ğŸ‘¤ You: ").strip()
            
            # Handle empty input
            if not question:
                print("âš ï¸  Please enter a question!")
                continue
            
            # Handle commands
            if question.lower() in ['quit', 'exit', 'bye', 'q']:
                print("\nğŸ‘‹ Thank you for using 1C Portal Support Chatbot!")
                print(f"ğŸ“Š You asked {conversation_count} questions in this session.")
                print("ğŸ’¡ Have a great day!")
                break
            
            elif question.lower() in ['help', '?']:
                print_help()
                continue
            
            elif question.lower() == 'info':
                print_info()
                continue
            
            elif question.lower() == 'examples':
                print_examples()
                continue
            
            elif question.lower() == 'clear':
                clear_screen()
                print_banner()
                continue
            
            # Process question
            conversation_count += 1
            print(f"\nğŸ” Searching knowledge base...")
            
            answer, sources = ask_question(question, show_debug=False)
            
            if answer:
                print(f"\nğŸ¤– Assistant:\n{answer}")
                
                if sources:
                    print(f"\nğŸ“„ Reference: Pages {', '.join(map(str, sources))} of the 1C Portal Support Guide")
                
                # Optional: Save conversation to log
                # log_conversation(question, answer, sources)
            else:
                print("\nâŒ Sorry, I couldn't generate an answer. Please try rephrasing your question.")
        
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ Chatbot interrupted. Goodbye!")
            break
        
        except Exception as e:
            print(f"\nâŒ An error occurred: {str(e)}")
            print("Please try again or type 'quit' to exit.")


if __name__ == "__main__":
    main()
```

---

## ğŸ¯ How to Run

### Step 1: Setup Configuration

1. Open `config.py` in PyCharm
2. Replace `YOUR_API_KEY_HERE` with your OpenAI API key
3. Save the file

### Step 2: Place Your PDF

1. Save the 1C Portal Support Guide as `1C_Portal_Support_Guide_v3.2.pdf`
2. Place it in the `data` folder

### Step 3: Create Vector Database

Run this command in PyCharm terminal:

```bash
python pdf_to_vectors.py
```

**Expected output:**
```
ğŸ“š 1C PORTAL RAG SYSTEM - VECTOR DATABASE CREATION
ğŸ“„ Reading PDF: data/1C_Portal_Support_Guide_v3.2.pdf
âœ… PDF read successfully!
âœ‚ï¸  Creating text chunks...
âœ… Created 250 chunks
ğŸ”„ Generating embeddings using OpenAI...
âœ… Embeddings generated!
ğŸ—‚ï¸  Creating FAISS vector index...
ğŸ’¾ Saving vector database...
ğŸ‰ VECTOR DATABASE CREATED SUCCESSFULLY!
```

This will create:
- `vector_db/vectors.index`
- `vector_db/chunks.pkl`

### Step 4: Run the Chatbot

```bash
python rag_chatbot.py
```

### Step 5: Start Asking Questions!

Example conversation:
```
ğŸ‘¤ You: How do I fill my timesheet?

ğŸ” Searching knowledge base...

ğŸ¤– Assistant:
To fill your weekly timesheet in the 1C Portal, follow these steps:

1. Navigate to: 1C Portal Home â†’ My Work â†’ Timesheets â†’ Current Timesheet
2. Select the time period (weekly: Monday to Sunday)
3. Click "Add Project" and search for your project by code or name
4. Enter work hours for each day (in 0.5 hour increments)
5. Add task descriptions for each entry
6. Review and validate your entries
7. Click "Submit for Approval"

The system will automatically check for errors like overtime hours, 
missing descriptions, or holiday conflicts before submission.

ğŸ“„ Reference: Pages 8, 9, 10 of the 1C Portal Support Guide
```

---

## ğŸ› Troubleshooting

### Error: "ModuleNotFoundError: No module named 'faiss'"

**Solution:**
```bash
pip uninstall faiss-cpu
pip install faiss-cpu
```

### Error: "OpenAI API key is invalid"

**Solution:**
1. Check your API key in `config.py`
2. Verify it starts with `sk-proj-` or `sk-`
3. Ensure you have credits in your OpenAI account

### Error: "PDF file not found"

**Solution:**
1. Check PDF is in `data/` folder
2. Verify filename matches: `1C_Portal_Support_Guide_v3.2.pdf`
3. Check file path in `config.py`

### Vector database not creating properly

**Solution:**
1. Delete `vector_db` folder contents
2. Re-run `python pdf_to_vectors.py`
3. Wait for completion (may take 5-10 minutes)

---

## ğŸ“ˆ Advanced Features (Optional)

### Add Conversation Logging

Add to `rag_chatbot.py`:

```python
def log_conversation(question, answer, sources):
    """Log conversation to file"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    with open("conversation_log.txt", "a", encoding="utf-8") as f:
        f.write(f"\n{'='*70}\n")
        f.write(f"Timestamp: {timestamp}\n")
        f.write(f"Question: {question}\n")
        f.write(f"Answer: {answer}\n")
        f.write(f"Sources: Pages {', '.join(map(str, sources))}\n")
```

### Improve Search Quality

Modify `TOP_K_RESULTS` in `config.py` (try values 3-7)

### Use Better Model

Change in `config.py`:
```python
CHAT_MODEL = "gpt-4"  # More accurate but slower and costlier
```

---

## ğŸ’° Cost Estimation

**For creating vector database:**
- 1C Portal Guide (~20 pages): ~$0.50-1.00

**For querying (per question):**
- Each question: ~$0.01-0.03
- 100 questions: ~$1-3

**Total for setup + 100 queries: ~$2-5**

---

## âœ… Testing Checklist

- [ ] Python 3.8+ installed
- [ ] PyCharm IDE setup
- [ ] All libraries installed
- [ ] OpenAI API key configured
- [ ] PDF placed in data folder
- [ ] Vector database created successfully
- [ ] Chatbot runs without errors
- [ ] Test questions return relevant answers

---

## ğŸ“ Support

If you encounter issues:
1. Check all file paths are correct
2. Verify OpenAI API key is valid
3
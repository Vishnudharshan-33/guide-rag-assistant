# RAG SYSTEM - DETAILED LINE-BY-LINE EXPLANATION
# RAG SYSTEM - DETAILED LINE-BY-LINE EXPLANATION
## Complete Technical Breakdown for Interview Preparation

---

## üéØ TABLE OF CONTENTS
1. Core Concepts & Architecture
2. PDF Processing Deep Dive
3. Vector Embeddings Explained
4. FAISS Index Deep Dive
5. Semantic Search Algorithm
6. Answer Generation Pipeline
7. Complete Annotated Code
8. Interview Questions & Answers

---

## 1. CORE RAG CONCEPTS

### What is RAG (Retrieval Augmented Generation)?

**RAG = Retrieval + Generation**

```
Traditional LLM:
Question ‚Üí LLM ‚Üí Answer
Problem: Limited to training data, no custom knowledge

RAG System:
Question ‚Üí Vector Search ‚Üí Retrieve Relevant Docs ‚Üí LLM + Context ‚Üí Answer
Benefit: Uses YOUR documents for accurate, updated answers
```

### RAG Architecture Components

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    RAG SYSTEM PIPELINE                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

OFFLINE PHASE (One-time setup):
1. PDF Document
   ‚Üì
2. Text Extraction (PyPDF2)
   ‚Üì
3. Text Chunking (Split into pieces)
   ‚Üì
4. Generate Embeddings (OpenAI API)
   ‚Üì
5. Store in Vector Database (FAISS)

ONLINE PHASE (Every query):
1. User Question
   ‚Üì
2. Convert to Embedding (OpenAI API)
   ‚Üì
3. Semantic Search (FAISS similarity)
   ‚Üì
4. Retrieve Top-K Chunks
   ‚Üì
5. Send to LLM with Context (GPT)
   ‚Üì
6. Generate Answer
```

---

## 2. DETAILED CODE WITH LINE-BY-LINE EXPLANATION

### Part 1: PDF to Vectors (Indexing Phase)

```python
# ============================================================================
# FILE: pdf_to_vectors_explained.py
# PURPOSE: Convert PDF to searchable vector database
# ============================================================================

# IMPORTS AND WHY WE NEED THEM
# ============================================================================

import faiss
# FAISS (Facebook AI Similarity Search)
# - Fast library for similarity search in high-dimensional spaces
# - Can search millions of vectors in milliseconds
# - Uses optimized algorithms (Product Quantization, HNSW)
# Technical: Implements approximate nearest neighbor (ANN) search

import openai
# OpenAI Python SDK
# - Communicates with OpenAI API
# - Provides embedding and chat completion endpoints
# - Handles authentication and request formatting

import PyPDF2
# PDF parsing library
# - Extracts text from PDF files
# - Handles different PDF encodings
# - Page-by-page text extraction

import numpy as np
# NumPy - Numerical Python
# - Efficient array operations
# - Required for vector mathematics
# - FAISS expects numpy arrays

import pickle
# Python serialization
# - Converts Python objects to binary format
# - Saves/loads complex data structures
# - Preserves data types and structure

import os
# Operating system interface
# - File path operations
# - Directory checking/creation
# - Cross-platform compatibility

from config import *
# Import configuration variables
# - API keys, file paths, hyperparameters
# - Centralized configuration management


# ============================================================================
# STEP 1: PDF TEXT EXTRACTION
# ============================================================================

def extract_text_from_pdf(pdf_path):
    """
    Extract text from PDF with metadata
    
    TECHNICAL DETAILS:
    - Opens PDF in binary mode ('rb')
    - Iterates through pages using PdfReader
    - Extracts text preserving page boundaries
    - Returns structured data with page numbers
    """
    
    print(f"üìÑ Opening PDF: {pdf_path}")
    
    # Open file in READ BINARY mode
    # Why binary? PDFs are binary files, not plain text
    with open(pdf_path, 'rb') as file:
        
        # Create PDF reader object
        # This parses the PDF structure
        pdf_reader = PyPDF2.PdfReader(file)
        
        # Get total pages using len()
        # PdfReader.pages returns a list-like object
        total_pages = len(pdf_reader.pages)
        print(f"Total pages: {total_pages}")
        
        # Initialize list to store page data
        pages_data = []
        
        # Iterate through each page
        # enumerate() gives us (index, object) tuples
        for page_num, page in enumerate(pdf_reader.pages):
            
            # Extract text from current page
            # extract_text() returns string
            # Handles different text encodings
            text = page.extract_text()
            
            # Store with metadata
            # Dictionary for structured data
            pages_data.append({
                'page_number': page_num + 1,  # 1-indexed for humans
                'text': text,
                'char_count': len(text)
            })
            
            # Progress indicator
            print(f"Extracted page {page_num + 1}/{total_pages}", end='\r')
        
        print()  # New line after progress
        
    return pages_data, total_pages


# ============================================================================
# STEP 2: TEXT CHUNKING STRATEGY
# ============================================================================

def create_smart_chunks(pages_data, chunk_size=500, overlap=100):
    """
    Split text into overlapping chunks
    
    WHY CHUNKING?
    1. LLMs have token limits (GPT-3.5: 4096 tokens)
    2. Embeddings work better on focused content
    3. Enables precise retrieval
    
    WHY OVERLAP?
    - Prevents context loss at boundaries
    - Example: "...end of sentence. New sentence..."
    - Without overlap, might split important context
    
    CHUNKING STRATEGIES:
    1. Fixed-size (what we use): Simple, consistent
    2. Sentence-based: Preserves complete thoughts
    3. Paragraph-based: Semantic boundaries
    4. Semantic: ML-based intelligent splitting
    """
    
    chunks = []
    metadata = []
    
    for page_info in pages_data:
        page_text = page_info['text']
        page_num = page_info['page_number']
        
        # SLIDING WINDOW APPROACH
        # Move through text with overlap
        # Example: chunk_size=500, overlap=100
        # Chunk 1: [0:500]
        # Chunk 2: [400:900]  (100 char overlap)
        # Chunk 3: [800:1300] (100 char overlap)
        
        start = 0
        while start < len(page_text):
            
            # Calculate end position
            end = start + chunk_size
            
            # Extract chunk
            chunk = page_text[start:end]
            
            # Skip very small chunks (less than 50 chars)
            # Why? Too small = not meaningful for search
            if len(chunk.strip()) > 50:
                
                # Store chunk
                chunks.append(chunk)
                
                # Store metadata for later reference
                metadata.append({
                    'chunk_id': len(chunks) - 1,
                    'page_number': page_num,
                    'start_pos': start,
                    'end_pos': end,
                    'chunk_size': len(chunk)
                })
            
            # Move window forward
            # Step = chunk_size - overlap
            # This creates the overlapping effect
            start += (chunk_size - overlap)
    
    print(f"‚úÇÔ∏è  Created {len(chunks)} chunks")
    print(f"üìä Avg chunk size: {sum(len(c) for c in chunks) // len(chunks)} chars")
    
    return chunks, metadata


# ============================================================================
# STEP 3: GENERATING EMBEDDINGS (THE MAGIC HAPPENS HERE)
# ============================================================================

def generate_embeddings(chunks):
    """
    Convert text chunks to vector embeddings
    
    WHAT ARE EMBEDDINGS?
    - Mathematical representation of text in vector space
    - Similar meanings ‚Üí Similar vectors
    - Example: "dog" and "puppy" have close vectors
    - "dog" and "car" have distant vectors
    
    EMBEDDING DIMENSIONS:
    - OpenAI text-embedding-ada-002: 1536 dimensions
    - Each dimension captures semantic features
    - Think: 1536 different aspects of meaning
    
    TECHNICAL PROCESS:
    Text ‚Üí Tokenization ‚Üí Neural Network ‚Üí 1536D Vector
    
    EXAMPLE:
    Input:  "How to submit timesheet?"
    Output: [0.023, -0.145, 0.892, ..., 0.334]  (1536 numbers)
    
    WHY EMBEDDINGS?
    - Enables semantic search (meaning-based, not keyword)
    - Mathematical operations (similarity = dot product)
    - Compact representation (text ‚Üí fixed-size vector)
    """
    
    embeddings = []
    batch_size = 100  # Process 100 chunks at a time
    
    print(f"üîÑ Generating embeddings for {len(chunks)} chunks...")
    
    # Process in batches to avoid API rate limits
    for i in range(0, len(chunks), batch_size):
        
        # Get batch of chunks
        batch = chunks[i:i + batch_size]
        
        try:
            # OPENAI API CALL
            # ============================================
            response = openai.Embedding.create(
                input=batch,  # Can send multiple texts
                model="text-embedding-ada-002"  # OpenAI's embedding model
            )
            # ============================================
            # WHAT HAPPENS IN API:
            # 1. Text ‚Üí Tokens (using BPE tokenization)
            # 2. Tokens ‚Üí Neural Network (transformer model)
            # 3. Network output ‚Üí 1536D vector (embedding)
            # 4. Vector normalized (length = 1)
            # ============================================
            
            # Extract embeddings from response
            # Response structure: {'data': [{'embedding': [...]}, ...]}
            for item in response['data']:
                embedding = item['embedding']  # List of 1536 floats
                embeddings.append(embedding)
            
            # Progress tracking
            progress = min(i + batch_size, len(chunks))
            print(f"Progress: {progress}/{len(chunks)} ({progress/len(chunks)*100:.1f}%)")
            
        except Exception as e:
            print(f"‚ùå Error in batch {i}: {str(e)}")
            # On error, add zero vectors as placeholder
            for _ in range(len(batch)):
                embeddings.append([0.0] * 1536)
    
    # Convert to NumPy array
    # Why? FAISS requires numpy arrays, not Python lists
    embeddings_array = np.array(embeddings, dtype='float32')
    
    print(f"‚úÖ Generated {len(embeddings)} embeddings")
    print(f"üìä Shape: {embeddings_array.shape}")  # (num_chunks, 1536)
    
    return embeddings_array


# ============================================================================
# STEP 4: CREATE FAISS INDEX (VECTOR DATABASE)
# ============================================================================

def create_faiss_index(embeddings):
    """
    Create FAISS index for fast similarity search
    
    WHAT IS FAISS INDEX?
    - Data structure optimized for similarity search
    - Like database index, but for vectors
    - Enables fast nearest neighbor search
    
    INDEX TYPES:
    1. IndexFlatL2: Exact search, L2 distance
       - Slowest but most accurate
       - Good for small datasets (<100K vectors)
    
    2. IndexFlatIP: Exact search, Inner Product (what we use)
       - For normalized vectors, IP = cosine similarity
       - Fast and accurate
    
    3. IndexIVFFlat: Approximate search with clustering
       - Much faster, slightly less accurate
       - Good for large datasets (>1M vectors)
    
    4. IndexHNSW: Hierarchical graph-based
       - Very fast queries
       - Good for production systems
    
    SIMILARITY METRICS:
    - L2 Distance: Euclidean distance
      dist = sqrt((v1[0]-v2[0])¬≤ + (v1[1]-v2[1])¬≤ + ...)
    
    - Inner Product (Dot Product):
      similarity = v1[0]*v2[0] + v1[1]*v2[1] + ...
      For normalized vectors: IP = Cosine Similarity
    
    - Cosine Similarity:
      similarity = dot(v1, v2) / (|v1| * |v2|)
      Range: [-1, 1], where 1 = identical, -1 = opposite
    """
    
    print("üóÇÔ∏è  Creating FAISS index...")
    
    # Get dimensionality (should be 1536)
    dimension = embeddings.shape[1]
    print(f"Vector dimension: {dimension}")
    
    # NORMALIZE VECTORS
    # ============================================
    # Why normalize?
    # 1. Makes all vectors unit length (length = 1)
    # 2. Inner product = Cosine similarity
    # 3. Better search quality
    # 
    # Mathematical operation:
    # normalized_v = v / ||v||
    # where ||v|| = sqrt(v[0]¬≤ + v[1]¬≤ + ... + v[n]¬≤)
    # ============================================
    faiss.normalize_L2(embeddings)
    print("‚úÖ Vectors normalized")
    
    # CREATE INDEX
    # ============================================
    # IndexFlatIP = Flat Index with Inner Product similarity
    # "Flat" means exhaustive search (checks all vectors)
    # "IP" means Inner Product metric
    # ============================================
    index = faiss.IndexFlatIP(dimension)
    
    # ADD VECTORS TO INDEX
    # ============================================
    # This builds the searchable data structure
    # For IndexFlatIP, it just stores the vectors
    # For advanced indexes, it builds clusters/graphs
    # ============================================
    index.add(embeddings)
    
    print(f"‚úÖ Index created with {index.ntotal} vectors")
    
    return index


# ============================================================================
# STEP 5: SAVE EVERYTHING TO DISK
# ============================================================================

def save_vector_database(index, chunks, metadata, total_pages):
    """
    Save index and data to disk
    
    WHY SAVE?
    - Avoid regenerating embeddings (costs money & time)
    - Enable offline querying
    - Persistent storage
    
    FILE FORMATS:
    1. vectors.index (FAISS binary format)
       - Optimized binary structure
       - Fast loading
       - FAISS-specific format
    
    2. chunks.pkl (Python pickle)
       - Serialized Python objects
       - Contains text and metadata
       - Easy to load in Python
    """
    
    print("üíæ Saving vector database...")
    
    # Create directory if doesn't exist
    os.makedirs("vector_db", exist_ok=True)
    
    # SAVE FAISS INDEX
    # ============================================
    # Uses FAISS's native binary format
    # Efficient storage and loading
    # ============================================
    faiss.write_index(index, "vector_db/vectors.index")
    print("‚úÖ Saved FAISS index")
    
    # SAVE CHUNKS AND METADATA
    # ============================================
    # Pickle: Python's serialization format
    # Converts Python objects ‚Üí bytes
    # Can save: lists, dicts, custom objects
    # ============================================
    data = {
        'chunks': chunks,              # List of text chunks
        'metadata': metadata,          # List of metadata dicts
        'total_pages': total_pages,    # Integer
        'embedding_model': 'text-embedding-ada-002',
        'created_at': str(datetime.now())
    }
    
    with open("vector_db/chunks.pkl", "wb") as f:
        pickle.dump(data, f)
    
    print("‚úÖ Saved chunks and metadata")


# ============================================================================
# MAIN EXECUTION
# ============================================================================

def main():
    """
    Complete pipeline: PDF ‚Üí Vectors ‚Üí Saved Index
    """
    
    pdf_path = "data/1C_Portal_Support_Guide.pdf"
    
    # Step 1: Extract text from PDF
    print("\n" + "="*70)
    print("STEP 1: PDF TEXT EXTRACTION")
    print("="*70)
    pages_data, total_pages = extract_text_from_pdf(pdf_path)
    
    # Step 2: Create chunks
    print("\n" + "="*70)
    print("STEP 2: TEXT CHUNKING")
    print("="*70)
    chunks, metadata = create_smart_chunks(pages_data)
    
    # Step 3: Generate embeddings
    print("\n" + "="*70)
    print("STEP 3: GENERATE EMBEDDINGS")
    print("="*70)
    embeddings = generate_embeddings(chunks)
    
    # Step 4: Create FAISS index
    print("\n" + "="*70)
    print("STEP 4: CREATE FAISS INDEX")
    print("="*70)
    index = create_faiss_index(embeddings)
    
    # Step 5: Save to disk
    print("\n" + "="*70)
    print("STEP 5: SAVE TO DISK")
    print("="*70)
    save_vector_database(index, chunks, metadata, total_pages)
    
    print("\nüéâ Vector database created successfully!")


if __name__ == "__main__":
    main()
```

---

## 3. QUERY PROCESSING (SEARCH PHASE)

```python
# ============================================================================
# FILE: query_processor_explained.py
# PURPOSE: Search vectors and generate answers
# ============================================================================

import faiss
import openai
import numpy as np
import pickle


# ============================================================================
# STEP 1: LOAD VECTOR DATABASE
# ============================================================================

def load_database():
    """
    Load saved FAISS index and chunks
    
    LOADING PROCESS:
    1. Read FAISS index from binary file
    2. Deserialize chunks from pickle
    3. Return all components
    """
    
    print("üìÇ Loading vector database...")
    
    # LOAD FAISS INDEX
    # ============================================
    # faiss.read_index() reads the binary format
    # Reconstructs the index data structure
    # ============================================
    index = faiss.read_index("vector_db/vectors.index")
    print(f"‚úÖ Loaded index with {index.ntotal} vectors")
    
    # LOAD CHUNKS AND METADATA
    # ============================================
    # pickle.load() deserializes the data
    # Recreates the original Python objects
    # ============================================
    with open("vector_db/chunks.pkl", "rb") as f:
        data = pickle.load(f)
    
    chunks = data['chunks']
    metadata = data['metadata']
    total_pages = data['total_pages']
    
    print(f"‚úÖ Loaded {len(chunks)} chunks from {total_pages} pages")
    
    return index, chunks, metadata, total_pages


# ============================================================================
# STEP 2: SEMANTIC SEARCH
# ============================================================================

def semantic_search(query, index, chunks, metadata, top_k=5):
    """
    Find most similar chunks to query
    
    SEMANTIC SEARCH ALGORITHM:
    1. Convert query to embedding (same space as documents)
    2. Calculate similarity with all document embeddings
    3. Return top-K most similar
    
    SIMILARITY CALCULATION:
    For normalized vectors:
    similarity = dot(query_vec, doc_vec)
    
    Example with 3D vectors (simplified):
    query_vec = [0.5, 0.3, 0.2]
    doc_vec   = [0.6, 0.4, 0.1]
    
    similarity = (0.5*0.6) + (0.3*0.4) + (0.2*0.1)
              = 0.30 + 0.12 + 0.02
              = 0.44
    
    Higher score = More similar
    Range: [-1, 1] for normalized vectors
    """
    
    print(f"üîç Searching for: '{query}'")
    
    # STEP 2.1: CONVERT QUERY TO EMBEDDING
    # ============================================
    # Same process as document embeddings
    # Must use same model (text-embedding-ada-002)
    # ============================================
    response = openai.Embedding.create(
        input=query,
        model="text-embedding-ada-002"
    )
    
    # Extract embedding and convert to numpy
    query_embedding = response['data'][0]['embedding']
    query_vector = np.array([query_embedding], dtype='float32')
    
    # Normalize query vector
    # Why? Index has normalized vectors
    # Must normalize query for consistent similarity
    faiss.normalize_L2(query_vector)
    
    print(f"‚úÖ Query converted to {query_vector.shape[1]}D vector")
    
    # STEP 2.2: SEARCH INDEX
    # ============================================
    # index.search() performs similarity search
    # 
    # Parameters:
    # - query_vector: The query embedding
    # - top_k: Number of results to return
    # 
    # Returns:
    # - scores: Similarity scores (higher = better)
    # - indices: Positions of matched chunks
    # ============================================
    scores, indices = index.search(query_vector, top_k)
    
    # TECHNICAL DETAILS OF SEARCH:
    # ============================================
    # For IndexFlatIP (what we use):
    # 1. Compute dot product with ALL vectors
    # 2. Sort by score (descending)
    # 3. Return top-K
    # 
    # Complexity: O(n*d) where n=vectors, d=dimensions
    # For 1000 vectors, 1536 dims: ~1.5M operations
    # Still very fast (< 1ms on modern CPU)
    # ============================================
    
    print(f"üéØ Found {len(indices[0])} similar chunks")
    
    # STEP 2.3: PREPARE RESULTS
    # ============================================
    results = []
    
    for score, idx in zip(scores[0], indices[0]):
        # Get chunk text and metadata
        chunk_text = chunks[idx]
        chunk_meta = metadata[idx]
        
        # Combine into result object
        results.append({
            'text': chunk_text,
            'score': float(score),  # Convert numpy float to Python float
            'page_number': chunk_meta['page_number'],
            'chunk_id': idx
        })
        
        # Debug output
        print(f"  üìÑ Page {chunk_meta['page_number']}, Score: {score:.3f}")
    
    return results


# ============================================================================
# STEP 3: ANSWER GENERATION WITH LLM
# ============================================================================

def generate_answer(query, search_results):
    """
    Generate answer using GPT with retrieved context
    
    RAG PROMPT ENGINEERING:
    1. System message: Define AI's role and behavior
    2. Context injection: Provide relevant chunks
    3. User query: The actual question
    4. Instructions: How to use context
    
    WHY THIS WORKS:
    - LLM has general knowledge (from training)
    - Context provides specific, recent information
    - LLM combines both for accurate answer
    """
    
    print("üí≠ Generating answer with GPT...")
    
    # STEP 3.1: BUILD CONTEXT
    # ============================================
    # Combine relevant chunks into single context
    # Include page numbers for citation
    # ============================================
    context_parts = []
    
    for result in search_results:
        # Only include chunks with reasonable similarity
        # Threshold: 0.5 (you can adjust this)
        if result['score'] > 0.5:
            page = result['page_number']
            text = result['text']
            
            # Format: [Page X]: chunk text
            context_parts.append(f"[Page {page}]:\n{text}")
    
    # Join all context parts
    context = "\n\n---\n\n".join(context_parts)
    
    # If no good matches found
    if not context_parts:
        return "I couldn't find relevant information in the document to answer this question."
    
    # STEP 3.2: CREATE PROMPT
    # ============================================
    # PROMPT STRUCTURE:
    # 1. System message (defines behavior)
    # 2. User message with context + question
    # ============================================
    
    system_message = """You are an AI assistant for the 1C Portal support system.

Your responsibilities:
1. Answer questions based ONLY on the provided context
2. Cite page numbers when giving information
3. Provide step-by-step instructions for processes
4. If information is not in context, say so clearly
5. Be professional and concise

Format your answers with:
- Numbered steps for procedures
- Bullet points for lists
- Clear section headers
"""
    
    user_message = f"""Context from 1C Portal Support Guide:

{context}

---

Question: {query}

Please provide a detailed answer based on the context above. Include relevant page numbers."""
    
    # STEP 3.3: CALL GPT API
    # ============================================
    # ChatCompletion API for conversational AI
    # ============================================
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",  # or "gpt-4" for better quality
        messages=[
            {"role": "system", "content": system_message},
            {"role": "user", "content": user_message}
        ],
        temperature=0.7,  # Creativity (0=deterministic, 1=creative)
        max_tokens=800    # Maximum response length
    )
    
    # WHAT HAPPENS IN GPT:
    # ============================================
    # 1. Tokenization: Text ‚Üí Tokens
    # 2. Attention mechanism: Focus on relevant parts
    # 3. Generation: Predict next token iteratively
    # 4. Decoding: Tokens ‚Üí Text
    # 
    # GPT doesn't "understand" like humans
    # It's predicting likely continuations
    # But with good prompts, produces amazing results!
    # ============================================
    
    # Extract answer from response
    answer = response.choices[0].message.content
    
    print("‚úÖ Answer generated")
    
    return answer


# ============================================================================
# STEP 4: COMPLETE RAG PIPELINE
# ============================================================================

def rag_query(query):
    """
    Complete RAG pipeline: Query ‚Üí Search ‚Üí Generate
    
    PIPELINE FLOW:
    1. Load database (vectors + chunks)
    2. Search for similar chunks
    3. Generate answer with context
    4. Return answer + sources
    """
    
    print("\n" + "="*70)
    print("RAG QUERY PIPELINE")
    print("="*70)
    
    # Step 1: Load database
    print("\n[1/3] Loading database...")
    index, chunks, metadata, total_pages = load_database()
    
    # Step 2: Semantic search
    print("\n[2/3] Performing semantic search...")
    results = semantic_search(query, index, chunks, metadata, top_k=5)
    
    # Step 3: Generate answer
    print("\n[3/3] Generating answer...")
    answer = generate_answer(query, results)
    
    # Extract source pages
    sources = sorted(set(r['page_number'] for r in results if r['score'] > 0.5))
    
    return answer, sources


# ============================================================================
# MAIN FUNCTION
# ============================================================================

def main():
    """
    Interactive query interface
    """
    
    print("ü§ñ RAG Chatbot Ready!")
    print("Ask questions about the 1C Portal")
    print("Type 'quit' to exit\n")
    
    while True:
        # Get user input
        query = input("üë§ You: ").strip()
        
        if query.lower() in ['quit', 'exit', 'q']:
            print("üëã Goodbye!")
            break
        
        if not query:
            continue
        
        # Process query
        answer, sources = rag_query(query)
        
        # Display results
        print(f"\nü§ñ Assistant:\n{answer}")
        
        if sources:
            print(f"\nüìÑ Sources: Pages {', '.join(map(str, sources))}")
        
        print("\n" + "-"*70 + "\n")


if __name__ == "__main__":
    main()
```

---

## 4. KEY TECHNICAL CONCEPTS

### A. Vector Embeddings Mathematics

```
TEXT EMBEDDING PROCESS:
======================

Input: "How to fill timesheet?"

Step 1: Tokenization
--------------------
"How" ‚Üí [482]
"to" ‚Üí [284]
"fill" ‚Üí [3fill]
"timesheet" ‚Üí [1532, 3443]
Result: [482, 284, 3fill, 1532, 3443]

Step 2: Neural Network Processing
---------------------------------
Transformer model with multiple layers:
- Attention mechanisms focus on relationships
- Feed-forward networks extract features
- Layer normalization for stability

Step 3: Embedding Output
-----------------------
1536-dimensional vector:
[0.0234, -0.1456, 0.8923, ..., 0.3341]

Each dimension represents semantic features:
- Dim 0-100: Grammatical structure
- Dim 101-500: General semantics
- Dim 501-1000: Domain-specific concepts
- Dim 1001-1536: Fine-grained meaning

Step 4: Normalization
--------------------
normalized_v = v / ||v||
Where ||v|| = sqrt(sum(v[i]¬≤))

This ensures all vectors have length = 1
Enables cosine similarity via dot product
```

### B. Similarity Search Deep Dive

```python
# SIMILARITY CALCULATION EXAMPLE
# ============================================

import numpy as np

# Example embeddings (simplified to 5D for illustration)
query_vec = np.array([0.5, 0.3, 0.2, 0.1, 0.4])
doc1_vec = np.array([0.6, 0.4, 0.1, 0.2, 0.3])
doc2_vec = np.array([0.1, 0.2, 0.8, 0.7, 0.1])

# Dot product (Inner Product)
similarity1 = np.dot(query_vec, doc1_vec)
# = 0.5*0.6 + 0.3*0.4 + 0.2*0.1 + 0.1*0.2 + 0.4*0.3
# = 0.30 + 0.12 + 0.02 + 0.02 + 0.12
# = 0.58

similarity2 = np.dot(query_vec, doc2_vec)
# = 0.5*0.1 + 0.3*0.2 + 0.2*0.8 + 0.1*0.7 + 0.4*0.1
# = 0.05 + 0.06 + 0.16 + 0.07 + 0.04
# = 0.38

# Doc1 is more similar (0.58 > 0.38)
```

### C. FAISS Index Types Comparison

```
INDEX TYPE COMPARISON:
=====================

1. IndexFlatIP (What we use)
   - Algorithm: Exhaustive search
   - Accuracy: 100% (exact)
   - Speed: O(n*d) - Linear with dataset size
   - Memory: Stores all vectors in RAM
   - Best for: <100K vectors
   - Query time: ~1ms for 10K vectors

2. IndexIVFFlat (Inverted File with Flat)
   - Algorithm: Clustering + search nearest clusters
   - Accuracy: 95-99% (configurable)
   - Speed: O(k*d) where k = clusters searched
   - Memory: Same as IndexFlat
   - Best for: 100K - 1M vectors
   - Query time: ~0.1ms for 1M vectors
   
   How it works:
   a. Training phase: Cluster vectors using k-means
   b. Search phase: Find nearest clusters, search within
   c. Trade-off: Speed vs accuracy (nprobe parameter)

3. IndexHNSW (Hierarchical Navigable Small World)
   - Algorithm: Graph-based navigation
   - Accuracy: 98-99.5%
   - Speed: O(log n) - Logarithmic!
   - Memory: Higher (stores graph structure)
   - Best for: >1M vectors, production systems
   - Query time: ~0.05ms even for 10M vectors
   
   How it works:
   a. Builds hierarchical graph
   b. Each node connected to nearest neighbors
   c. Search navigates graph layer by layer
   d. Very fast, great for real-time applications

4. IndexPQ (Product Quantization)
   - Algorithm: Vector compression
   - Accuracy: 90-95%
   - Speed: Fast
   - Memory: 32-64x compression!
   - Best for: Very large datasets with limited RAM
   - Query time: ~0.5ms for 10M vectors
   
   How it works:
   a. Divides vector into sub-vectors
   b. Quantizes each sub-vector separately
   c. Stores codes instead of full vectors
   d. Approximate distance calculation
```

---

## 5. INTERVIEW PREPARATION

### A. Common RAG Interview Questions

```
Q1: What is RAG and why do we need it?
======================================

ANSWER:
RAG stands for Retrieval Augmented Generation. It's a technique that 
enhances LLM responses by retrieving relevant information from external 
knowledge sources.

Why we need it:
1. LLMs have knowledge cutoff dates (trained on old data)
2. LLMs can't access private/proprietary documents
3. LLMs hallucinate when they don't know
4. RAG provides factual, up-to-date, domain-specific information

Example: Without RAG, ChatGPT can't answer questions about your 
company's internal policies. With RAG, it can retrieve and reference 
your policy documents.

---

Q2: Explain the RAG pipeline in detail
======================================

ANSWER:
RAG has two phases:

INDEXING PHASE (Offline):
1. Document Loading: Read PDFs, docs, web pages
2. Text Splitting: Break into chunks (500-1000 chars)
3. Embedding Generation: Convert chunks to vectors using LLM
4. Vector Storage: Store in vector database (FAISS, Pinecone)

QUERY PHASE (Online):
1. Query Embedding: Convert user question to vector
2. Similarity Search: Find most relevant chunks (cosine similarity)
3. Context Preparation: Combine top-K chunks
4. Answer Generation: Send context + query to LLM
5. Response: LLM generates answer based on context

Key insight: Query and documents in same vector space enables semantic search.

---

Q3: What are embeddings and how do they work?
=============================================

ANSWER:
Embeddings are dense vector representations of text where similar meanings 
have similar vectors.

Technical details:
- Dimension: Typically 768-1536 dimensions
- Generation: Neural networks (transformers) convert text ‚Üí vector
- Properties: Captures semantic relationships
  Example: vec("king") - vec("man") + vec("woman") ‚âà vec("queen")

How they work:
1. Text ‚Üí Tokenization (words ‚Üí numbers)
2. Tokens ‚Üí Embedding layer (lookup table)
3. Transformer layers (attention + feed-forward)
4. Final hidden state ‚Üí embedding vector

Why useful:
- Enables semantic search (meaning-based, not keyword)
- Mathematical operations (similarity = dot product)
- Compact representation (any text ‚Üí fixed size)

Models: OpenAI Ada-002, Google PaLM, Sentence-BERT, etc.

---

Q4: How does semantic search differ from keyword search?
========================================================

ANSWER:

KEYWORD SEARCH (Traditional):
- Matches exact words/tokens
- Example: Query "car" won't match "automobile"
- Technology: Inverted index, TF-IDF, BM25
- Fast but limited

SEMANTIC SEARCH (RAG):
- Matches meaning/intent
- Example: "car" and "automobile" are similar vectors
- Technology: Neural embeddings + vector similarity
- More accurate for natural language

Example:
Query: "How to submit hours worked?"
Keyword: Needs exact match "submit hours"
Semantic: Matches "timesheet submission", "log work time", etc.

Implementation:
1. Convert query + docs to embeddings
2. Calculate cosine similarity
3. Return highest scoring documents

Code:
similarity = dot(query_vec, doc_vec) / (|query_vec| * |doc_vec|)

---

Q5: What is chunking and why is it important?
=============================================

ANSWER:
Chunking is splitting long documents into smaller pieces.

Why chunk:
1. Token limits: GPT-3.5 = 4K tokens (~3K words)
2. Focus: Smaller chunks = more precise retrieval
3. Cost: Embedding APIs charge per token
4. Performance: Smaller = faster similarity computation

Chunking strategies:

1. FIXED SIZE (what we use):
   - Split every N characters (e.g., 500 chars)
   - Pros: Simple, consistent
   - Cons: May split sentences/paragraphs
   
   chunk_size = 500
   overlap = 100  # Preserve context at boundaries
   chunks = [text[i:i+500] for i in range(0, len(text), 400)]

2. SENTENCE-BASED:
   - Split at sentence boundaries
   - Pros: Preserves complete thoughts
   - Cons: Variable chunk sizes
   
   Uses NLP libraries (spaCy, NLTK)

3. PARAGRAPH-BASED:
   - Split at paragraphs/sections
   - Pros: Semantic boundaries
   - Cons: Large size variations

4. SEMANTIC:
   - ML model determines natural breakpoints
   - Pros: Most intelligent
   - Cons: Complex, slow

Best practice: Fixed size with overlap (100-200 char overlap)

---

Q6: Explain FAISS and why we use it
===================================

ANSWER:
FAISS = Facebook AI Similarity Search

What it does:
- Efficient nearest neighbor search in high-dimensional spaces
- Optimized for billion-scale vector searches
- Multiple index types for different use cases

Why FAISS:
1. Speed: Can search millions of vectors in milliseconds
2. Scalability: Handles billion-scale datasets
3. Flexibility: Many index types (flat, IVF, HNSW)
4. Efficiency: Optimized C++ with Python bindings
5. Free: Open source

Alternatives:
- Pinecone: Managed, cloud-based
- Milvus: Distributed, production-ready
- Weaviate: GraphQL, hybrid search
- ChromaDB: Simple, embedded

FAISS vs Others:
- FAISS: Best for local, high performance
- Pinecone: Best for managed, scalable production
- ChromaDB: Best for simple, quick prototypes

Code example:
import faiss
index = faiss.IndexFlatIP(1536)  # Inner product
index.add(embeddings)  # Add vectors
scores, indices = index.search(query_vec, k=5)  # Search

---

Q7: How do you handle long documents that exceed token limits?
==============================================================

ANSWER:
Multiple strategies:

1. MAP-REDUCE APPROACH:
   - Split document into chunks
   - Process each chunk separately
   - Combine results
   
   Example:
   chunk1 ‚Üí summarize ‚Üí summary1
   chunk2 ‚Üí summarize ‚Üí summary2
   [summary1, summary2] ‚Üí final summary

2. REFINE APPROACH:
   - Process first chunk ‚Üí answer1
   - Process second chunk with answer1 ‚Üí answer2
   - Iteratively refine
   
   answer = ""
   for chunk in chunks:
       answer = llm(chunk + answer)

3. RAG APPROACH (what we use):
   - Retrieve only relevant chunks
   - Send top-K to LLM (fits in context)
   - Most efficient
   
   relevant = search(query, chunks, top_k=5)
   answer = llm(relevant + query)

4. HIERARCHICAL SUMMARIZATION:
   - Level 1: Summarize paragraphs
   - Level 2: Summarize summaries
   - Creates condensed representation

Best for RAG: Retrieve only what's needed

---

Q8: What is the difference between fine-tuning and RAG?
=======================================================

ANSWER:

FINE-TUNING:
- Training: Update model weights on your data
- Knowledge: Baked into model parameters
- Updates: Requires retraining (expensive)
- Best for: Style, format, domain adaptation
- Cost: High (training GPUs, time)
- Latency: Fast inference
- Example: Train GPT on medical papers

RAG:
- Training: No model training needed
- Knowledge: Retrieved from external DB
- Updates: Just update documents (cheap)
- Best for: Dynamic, factual information
- Cost: Low (just API calls)
- Latency: Slightly slower (extra retrieval step)
- Example: Search docs + generate answer

When to use:
- Fine-tuning: Custom behavior, specific writing style
- RAG: Factual QA, document search, up-to-date info
- Both: Fine-tune for domain, RAG for facts

Hybrid approach (best):
Fine-tuned model + RAG = Domain expertise + Current facts

---

Q9: How do you evaluate RAG system performance?
===============================================

ANSWER:

METRICS:

1. RETRIEVAL METRICS:
   a. Precision@K: How many retrieved docs are relevant?
      precision = relevant_retrieved / total_retrieved
   
   b. Recall@K: How many relevant docs were retrieved?
      recall = relevant_retrieved / total_relevant
   
   c. MRR (Mean Reciprocal Rank): Position of first relevant doc
      MRR = 1 / rank_of_first_relevant

2. GENERATION METRICS:
   a. Faithfulness: Answer consistent with retrieved context?
   b. Answer Relevance: Answer addresses the question?
   c. Context Relevance: Retrieved chunks relevant to query?

3. END-TO-END METRICS:
   a. BLEU: N-gram overlap with reference answers
   b. ROUGE: Recall-oriented, for summarization
   c. Human Evaluation: Gold standard

EVALUATION PROCESS:
1. Create test set: (query, ground_truth_answer) pairs
2. Run RAG pipeline on queries
3. Compare generated vs ground truth
4. Calculate metrics

Code example:
```python
def evaluate_rag(test_queries, ground_truths):
    scores = []
    for query, truth in zip(test_queries, ground_truths):
        answer, sources = rag_query(query)
        score = calculate_similarity(answer, truth)
        scores.append(score)
    return np.mean(scores)
```

Tools: RAGAS, TruLens, LangChain evaluators

---

Q10: What are common challenges in RAG and solutions?
====================================================

ANSWER:

CHALLENGES & SOLUTIONS:

1. IRRELEVANT RETRIEVAL:
   Problem: System retrieves unrelated chunks
   Solutions:
   - Better chunking strategy (semantic boundaries)
   - Hybrid search (keyword + semantic)
   - Query rewriting/expansion
   - Adjust top-K parameter
   - Use reranking models

2. HALLUCINATION:
   Problem: LLM makes up information
   Solutions:
   - Strong system prompts ("answer ONLY from context")
   - Citation requirements (force page references)
   - Confidence scoring
   - Human-in-the-loop verification

3. CONTEXT TOO LONG:
   Problem: Retrieved chunks exceed token limit
   Solutions:
   - Reduce top-K
   - Reranking (get top-20, rerank to top-5)
   - Compression (summarize chunks)
   - Sliding window approach

4. SLOW RESPONSE:
   Problem: Embedding + search + generation takes time
   Solutions:
   - Caching frequent queries
   - Async processing
   - Batch embedding generation
   - Faster index types (HNSW)
   - Smaller, faster LLMs

5. DOCUMENT UPDATES:
   Problem: Need to reindex frequently
   Solutions:
   - Incremental indexing
   - Versioned embeddings
   - Delta updates (add/remove specific chunks)
   - Scheduled batch updates

6. CROSS-LINGUAL:
   Problem: Query in English, docs in multiple languages
   Solutions:
   - Multilingual embeddings (mBERT, XLM-R)
   - Translate at query time
   - Language-specific indexes
```

---

## 6. ADVANCED CONCEPTS

### A. Hybrid Search Implementation

```python
"""
HYBRID SEARCH: Combines keyword + semantic search
Better accuracy than either alone
"""

from rank_bm25 import BM25Okapi  # Keyword search
import faiss  # Semantic search
import numpy as np


def hybrid_search(query, chunks, embeddings, index, top_k=10):
    """
    Combines BM25 (keyword) and FAISS (semantic) scores
    """
    
    # 1. KEYWORD SEARCH (BM25)
    # ========================
    # BM25: Best Matching 25 algorithm
    # Improved TF-IDF with document length normalization
    
    # Tokenize chunks
    tokenized_chunks = [chunk.lower().split() for chunk in chunks]
    
    # Create BM25 index
    bm25 = BM25Okapi(tokenized_chunks)
    
    # Search
    tokenized_query = query.lower().split()
    bm25_scores = bm25.get_scores(tokenized_query)
    
    # 2. SEMANTIC SEARCH (FAISS)
    # ==========================
    query_embedding = get_embedding(query)
    query_vec = np.array([query_embedding]).astype('float32')
    faiss.normalize_L2(query_vec)
    
    semantic_scores, indices = index.search(query_vec, len(chunks))
    semantic_scores = semantic_scores[0]
    
    # 3. COMBINE SCORES
    # =================
    # Normalize both scores to [0, 1]
    bm25_normalized = bm25_scores / (max(bm25_scores) + 1e-6)
    semantic_normalized = (semantic_scores + 1) / 2  # [-1,1] ‚Üí [0,1]
    
    # Weighted combination (adjust weights as needed)
    alpha = 0.5  # Weight for semantic
    beta = 0.5   # Weight for keyword
    
    combined_scores = alpha * semantic_normalized + beta * bm25_normalized
    
    # Get top-K
    top_indices = np.argsort(combined_scores)[-top_k:][::-1]
    
    results = []
    for idx in top_indices:
        results.append({
            'chunk': chunks[idx],
            'score': combined_scores[idx],
            'bm25_score': bm25_scores[idx],
            'semantic_score': semantic_scores[idx]
        })
    
    return results
```

### B. Query Rewriting for Better Retrieval

```python
"""
QUERY REWRITING: Reformulate query for better retrieval
"""

def rewrite_query(original_query):
    """
    Uses LLM to generate better search queries
    """
    
    prompt = f"""Given the user query, generate 3 alternative phrasings that 
would help retrieve relevant information from a knowledge base.

Original query: {original_query}

Generate queries that are:
1. More specific
2. Use different keywords
3. Address the same intent

Format: Return only the 3 queries, one per line."""
    
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7
    )
    
    # Parse response into multiple queries
    rewritten = response.choices[0].message.content.split('\n')
    rewritten = [q.strip() for q in rewritten if q.strip()]
    
    return [original_query] + rewritten


def multi_query_search(query, index, chunks, metadata):
    """
    Search with multiple query variations
    """
    
    # Generate query variations
    queries = rewrite_query(query)
    
    all_results = []
    
    for q in queries:
        results = semantic_search(q, index, chunks, metadata, top_k=5)
        all_results.extend(results)
    
    # Deduplicate and rerank
    unique_chunks = {}
    for result in all_results:
        chunk_id = result['chunk_id']
        if chunk_id not in unique_chunks:
            unique_chunks[chunk_id] = result
        else:
            # Keep highest score
            if result['score'] > unique_chunks[chunk_id]['score']:
                unique_chunks[chunk_id] = result
    
    # Sort by score
    final_results = sorted(
        unique_chunks.values(), 
        key=lambda x: x['score'], 
        reverse=True
    )[:10]
    
    return final_results
```

### C. Reranking for Better Accuracy

```python
"""
RERANKING: Use cross-encoder to rerank retrieved results
More accurate but slower than bi-encoder (embeddings)
"""

from sentence_transformers import CrossEncoder


def rerank_results(query, results, top_k=5):
    """
    Rerank using cross-encoder model
    
    DIFFERENCE:
    - Bi-encoder (embeddings): Encodes query and docs separately
      Fast but less accurate
    
    - Cross-encoder (reranker): Encodes query+doc together
      Slow but more accurate
    """
    
    # Load cross-encoder model
    model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
    
    # Prepare pairs for reranking
    pairs = [[query, result['text']] for result in results]
    
    # Get relevance scores
    scores = model.predict(pairs)
    
    # Add scores to results
    for i, result in enumerate(results):
        result['rerank_score'] = scores[i]
    
    # Sort by rerank score
    reranked = sorted(results, key=lambda x: x['rerank_score'], reverse=True)
    
    return reranked[:top_k]


# USAGE IN RAG:
# =============
# 1. Retrieve top-20 with bi-encoder (fast)
# 2. Rerank to top-5 with cross-encoder (accurate)
# 3. Send top-5 to LLM

def improved_rag_query(query):
    # Step 1: Fast retrieval (top-20)
    candidates = semantic_search(query, index, chunks, metadata, top_k=20)
    
    # Step 2: Accurate reranking (top-5)
    top_results = rerank_results(query, candidates, top_k=5)
    
    # Step 3: Generate answer
    answer = generate_answer(query, top_results)
    
    return answer
```

---

## 7. PRODUCTION BEST PRACTICES

### A. Error Handling

```python
"""
ROBUST ERROR HANDLING FOR PRODUCTION
"""

import logging
import time
from functools import wraps

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def retry_with_backoff(max_retries=3, base_delay=1):
    """
    Decorator for retrying failed API calls
    Exponential backoff: wait 1s, 2s, 4s, 8s...
    """
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    if attempt == max_retries - 1:
                        logger.error(f"Failed after {max_retries} attempts: {e}")
                        raise
                    
                    delay = base_delay * (2 ** attempt)
                    logger.warning(f"Attempt {attempt + 1} failed. Retrying in {delay}s...")
                    time.sleep(delay)
        return wrapper
    return decorator


@retry_with_backoff(max_retries=3)
def generate_embedding_robust(text):
    """
    Generate embedding with error handling
    """
    try:
        response = openai.Embedding.create(
            input=text,
            model="text-embedding-ada-002"
        )
        return response['data'][0]['embedding']
    
    except openai.error.RateLimitError:
        logger.error("Rate limit exceeded. Wait and retry.")
        raise
    
    except openai.error.APIError as e:
        logger.error(f"OpenAI API error: {e}")
        raise
    
    except Exception as e:
        logger.error(f"Unexpected error: {e}")
        raise


def safe_rag_query(query):
    """
    RAG query with comprehensive error handling
    """
    try:
        # Validate input
        if not query or len(query.strip()) < 3:
            return "Please provide a valid question (at least 3 characters).", []
        
        # Load database
        try:
            index, chunks, metadata, total_pages = load_database()
        except FileNotFoundError:
            return "Database not found. Please run indexing first.", []
        except Exception as e:
            logger.error(f"Database loading error: {e}")
            return "Error loading database. Please contact support.", []
        
        # Search
        try:
            results = semantic_search(query, index, chunks, metadata)
        except Exception as e:
            logger.error(f"Search error: {e}")
            return "Error during search. Please try again.", []
        
        # Generate answer
        try:
            answer = generate_answer(query, results)
            sources = [r['page_number'] for r in results if r['score'] > 0.5]
            return answer, sources
        except Exception as e:
            logger.error(f"Answer generation error: {e}")
            return "Error generating answer. Please rephrase your question.", []
    
    except Exception as e:
        logger.error(f"Unexpected error in RAG query: {e}")
        return "An unexpected error occurred. Please try again.", []
```

### B. Caching for Performance

```python
"""
CACHING TO IMPROVE PERFORMANCE AND REDUCE COSTS
"""

import hashlib
import json
from functools import lru_cache


class EmbeddingCache:
    """
    Cache embeddings to avoid redundant API calls
    """
    
    def __init__(self, cache_file="embedding_cache.json"):
        self.cache_file = cache_file
        self.cache = self._load_cache()
    
    def _load_cache(self):
        try:
            with open(self.cache_file, 'r') as f:
                return json.load(f)
        except FileNotFoundError:
            return {}
    
    def _save_cache(self):
        with open(self.cache_file, 'w') as f:
            json.dump(self.cache, f)
    
    def get_cache_key(self, text):
        """Generate unique key for text"""
        return hashlib.md5(text.encode()).hexdigest()
    
    def get_embedding(self, text):
        """Get embedding from cache or API"""
        key = self.get_cache_key(text)
        
        if key in self.cache:
            logger.info("Cache hit! Using cached embedding.")
            return self.cache[key]
        
        logger.info("Cache miss. Calling API...")
        embedding = generate_embedding_robust(text)
        
        self.cache[key] = embedding
        self._save_cache()
        
        return embedding


# Query result caching
@lru_cache(maxsize=100)
def cached_rag_query(query):
    """
    Cache recent query results
    LRU = Least Recently Used
    Keeps 100 most recent queries in memory
    """
    return rag_query(query)


# Usage:
cache = EmbeddingCache()
embedding = cache.get_embedding("How to fill timesheet?")
```

---

## 8. COMPLETE WORKING EXAMPLE

### Final Production-Ready Code

```python
"""
=================================================================
PRODUCTION-READY RAG SYSTEM
Complete implementation with all best practices
=================================================================
"""

import faiss
import openai
import PyPDF2
import numpy as np
import pickle
import os
import logging
from typing import List, Dict, Tuple
from dataclasses import dataclass

# Configuration
openai.api_key = "your-api-key"
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class RAGConfig:
    """Configuration dataclass"""
    pdf_path: str = "data/document.pdf"
    vector_index_path: str = "vector_db/index.faiss"
    chunks_path: str = "vector_db/chunks.pkl"
    chunk_size: int = 500
    chunk_overlap: int = 100
    embedding_model: str = "text-embedding-ada-002"
    chat_model: str = "gpt-3.5-turbo"
    top_k: int = 5


class RAGSystem:
    """
    Complete RAG system with indexing and querying
    """
    
    def __init__(self, config: RAGConfig):
        self.config = config
        self.index = None
        self.chunks = None
        self.metadata = None
    
    
    # ==================== INDEXING ====================
    
    def build_index(self):
        """
        Complete indexing pipeline
        """
        logger.info("Starting indexing pipeline...")
        
        # Step 1: Extract text
        pages_data = self._extract_text()
        
        # Step 2: Create chunks
        self.chunks, self.metadata = self._create_chunks(pages_data)
        
        # Step 3: Generate embeddings
        embeddings = self._generate_embeddings(self.chunks)
        
        # Step 4: Build FAISS index
        self.index = self._build_faiss_index(embeddings)
        
        # Step 5: Save
        self._save_index()
        
        logger.info("‚úÖ Indexing complete!")
    
    def _extract_text(self) -> List[Dict]:
        """Extract text from PDF"""
        with open(self.config.pdf_path, 'rb') as f:
            reader = PyPDF2.PdfReader(f)
            pages = []
            for i, page in enumerate(reader.pages):
                pages.append({
                    'page_number': i + 1,
                    'text': page.extract_text()
                })
        return pages
    
    def _create_chunks(self, pages_data) -> Tuple[List[str], List[Dict]]:
        """Create overlapping chunks"""
        chunks, metadata = [], []
        
        for page in pages_data:
            text = page['text']
            page_num = page['page_number']
            
            for i in range(0, len(text), self.config.chunk_size - self.config.chunk_overlap):
                chunk = text[i:i + self.config.chunk_size]
                if len(chunk.strip()) > 50:
                    chunks.append(chunk)
                    metadata.append({'page_number': page_num, 'chunk_id': len(chunks) - 1})
        
        return chunks, metadata
    
    def _generate_embeddings(self, chunks) -> np.ndarray:
        """Generate embeddings for chunks"""
        embeddings = []
        batch_size = 100
        
        for i in range(0, len(chunks), batch_size):
            batch = chunks[i:i + batch_size]
            response = openai.Embedding.create(
                input=batch,
                model=self.config.embedding_model
            )
            embeddings.extend([item['embedding'] for item in response['data']])
        
        return np.array(embeddings, dtype='float32')
    
    def _build_faiss_index(self, embeddings) -> faiss.Index:
        """Build FAISS index"""
        faiss.normalize_L2(embeddings)
        index = faiss.IndexFlatIP(embeddings.shape[1])
        index.add(embeddings)
        return index
    
    def _save_index(self):
        """Save index and chunks"""
        os.makedirs(os.path.dirname(self.config.vector_index_path), exist_ok=True)
        faiss.write_index(self.index, self.config.vector_index_path)
        
        with open(self.config.chunks_path, 'wb') as f:
            pickle.dump({
                'chunks': self.chunks,
                'metadata': self.metadata
            }, f)
    
    
    # ==================== QUERYING ====================
    
    def load_index(self):
        """Load saved index"""
        self.index = faiss.read_index(self.config.vector_index_path)
        
        with open(self.config.chunks_path, 'rb') as f:
            data = pickle.load(f)
            self.chunks = data['chunks']
            self.metadata = data['metadata']
    
    def query(self, question: str) -> Tuple[str, List[int]]:
        """
        Query the RAG system
        Returns: (answer, source_pages)
        """
        # Ensure index is loaded
        if self.index is None:
            self.load_index()
        
        # Search
        results = self._search(question)
        
        # Generate answer
        answer = self._generate_answer(question, results)
        
        # Extract sources
        sources = sorted(set(r['page_number'] for r in results if r['score'] > 0.5))
        
        return answer, sources
    
    def _search(self, query: str) -> List[Dict]:
        """Semantic search"""
        # Get query embedding
        response = openai.Embedding.create(
            input=query,
            model=self.config.embedding_model
        )
        query_vec = np.array([response['data'][0]['embedding']], dtype='float32')
        faiss.normalize_L2(query_vec)
        
        # Search
        scores, indices = self.index.search(query_vec, self.config.top_k)
        
        results = []
        for score, idx in zip(scores[0], indices[0]):
            results.append({
                'text': self.chunks[idx],
                'score': float(score),
                'page_number': self.metadata[idx]['page_number']
            })
        
        return results
    
    def _generate_answer(self, query: str, results: List[Dict]) -> str:
        """Generate answer with LLM"""
        # Build context
        context = "\n\